{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper - Find all the jobs for Data Science across the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add in all import statements\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from time import time, sleep\n",
    "from tqdm import tqdm\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "from easymoney.money import EasyPeasy \n",
    "import pycountry\n",
    "import requests\n",
    "import wbdata\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force Method\n",
    "This is done as the indeed links across various countries do not follow the same general structure for indeed. There is a far more elegant way to do this if the urls are the same across the board, however this will have to do for now. This scrape will take the following information from each site: City, Job title, posted date, salary, and the company to which the job posting has come from. The code is essentially the same for each of the cities but varies a bit from country to country to allow for the variation that exists in the architecture of each site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brute force all the urls to scrape in Indeed\n",
    "url = \"https://ca.indeed.com/jobs?q=Data+Scientist&l=Vancouver%2C+BC&sort=date&start={}\"\n",
    "url_2 = \"https://ca.indeed.com/jobs?q=Data+Scientist&l=Toronto&sort=date&start={}\"\n",
    "url_3 = \"https://www.indeed.com/jobs?q=data+scientist&l=Seattle%2C+WA&start={}\"\n",
    "url_4 = \"https://ca.indeed.com/jobs?q=data+scientist&l=Montr%C3%A9al%2C+QC&start={}\"\n",
    "url_5 = \"https://www.indeed.com/jobs?q=data+scientist&l=San+Francisco%2C+CA&start={}\"\n",
    "url_6 = \"https://www.indeed.com/jobs?q=data+scientist&l=austin+texas&start={}\"\n",
    "url_7 = \"https://www.indeed.co.in/jobs?q=data+scientist&l=Bengaluru%2C+Karnataka&sort=date&start={}\"\n",
    "url_8 = \"https://www.indeed.co.in/jobs?q=Data+Scientist&l=Mumbai%2C+Maharashtra&start={}\"\n",
    "url_9 = \"https://www.indeed.com/jobs?q=Data+Science&l=united+states&start={}\"\n",
    "url_10 = \"https://www.indeed.com/jobs?q=Data+Engineer&l=united+states&start={}\"\n",
    "url_11 = \"https://www.indeed.com/jobs?q=data+analyst&l=united+states&start={}\"\n",
    "url_12 = \"https://ca.indeed.com/jobs?q=Data+Analyst&start={}\"\n",
    "url_13 = \"https://ca.indeed.com/jobs?q=data+scientist&start={}\"\n",
    "url_14 = \"https://www.indeed.co.uk/jobs?q=Data+Scientist&l=London&start={}\"\n",
    "url_15 = \"https://dk.indeed.com/jobs?q=Data+Scientist&l=K%C3%B8benhavn&start={}\"\n",
    "url_16 = \"https://se.indeed.com/jobs?q=Data+Scientist&l=Stockholm&start={}\"\n",
    "#Go on an scrape each of the cities in a brute force manner.\n",
    "job_title = []\n",
    "date_posted = []\n",
    "location = []\n",
    "salary = []\n",
    "company = []\n",
    "#Find a way to automate the length of this for loop to go through all the pages...\n",
    "#Vancouver\n",
    "for x in tqdm(range(10,540,10)):\n",
    "    van = url.format(x)\n",
    "    r = requests.get(van)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    #Start from here to add the cards The issue with this script is here....\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('Vancouver BC')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "#Toronto\n",
    "for x in tqdm(range(10,1200,10)):\n",
    "    tor = url_2.format(x)\n",
    "    r_2 = requests.get(tor)\n",
    "    soup = BeautifulSoup(r_2.text, 'html.parser')\n",
    "    #Start from here to add the cards The issue with this script is here....\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('Toronto ON')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#Seattle - to fix, slight pain\n",
    "for x in tqdm(range(10,8580,10)):\n",
    "    sea = url_3.format(x)\n",
    "    r_3 = requests.get(sea)\n",
    "    soup = BeautifulSoup(r_3.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('Seattle WA')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#Montreal - to fix - slight pain\n",
    "for x in tqdm(range(10,780,10)):\n",
    "    mtl = url_4.format(x)\n",
    "    r_4 = requests.get(mtl)\n",
    "    soup = BeautifulSoup(r_4.text, 'html.parser')\n",
    "    #Start from here to add the cards The issue with this script is here....\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('Montreal QC')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "for x in tqdm(range(10,5630,10)):\n",
    "    sfo = url_5.format(x)\n",
    "    r_5 = requests.get(sfo)\n",
    "    soup = BeautifulSoup(r_5.text, 'html.parser')\n",
    "    #Start from here to add the cards The issue with this script is here....\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('San Francisco CA')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#Austin - to fix\n",
    "for x in tqdm(range(10,1350,10)):\n",
    "    aus = url_6.format(x)\n",
    "    r_6 = requests.get(aus)\n",
    "    soup = BeautifulSoup(r_6.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('Austin TX')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#Bengaluru\n",
    "for x in tqdm(range(10,2000,10)):\n",
    "    blr = url_7.format(x)\n",
    "    r_7 = requests.get(blr)\n",
    "    soup = BeautifulSoup(r_7.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('Bangalore KA')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#Mumbai\n",
    "for x in tqdm(range(10,1000,10)):\n",
    "    bom = url_8.format(x)\n",
    "    r_8 = requests.get(bom)\n",
    "    soup = BeautifulSoup(r_8.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('Mumbai MH')\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#United States\n",
    "for x in tqdm(range(10,8000,10)):\n",
    "    usa = url_9.format(x)\n",
    "    r_9 = requests.get(usa)\n",
    "    soup = BeautifulSoup(r_9.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('div', attrs={'class':\"location accessible-contrast-color-location\"})\n",
    "        except:\n",
    "            location.append(np.nan)\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append('USA')\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#Data Engineer USA\n",
    "for x in tqdm(range(10,8000,10)):\n",
    "    us = url_10.format(x)\n",
    "    r_10 = requests.get(us)\n",
    "    soup = BeautifulSoup(r_10.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('div', attrs={'class':\"location accessible-contrast-color-location\"})\n",
    "        except:\n",
    "            location.append('USA')\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#Data Analyst USA\n",
    "for x in tqdm(range(10,8000,10)):\n",
    "    idk = url_11.format(x)\n",
    "    r_11 = requests.get(idk)\n",
    "    soup = BeautifulSoup(r_11.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('div', attrs={'class':\"location accessible-contrast-color-location\"})\n",
    "        except:\n",
    "            location.append('USA')\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#can\n",
    "for x in tqdm(range(10,7000,10)):\n",
    "    can = url_12.format(x)\n",
    "    r_12 = requests.get(can)\n",
    "    soup = BeautifulSoup(r_12.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('div', attrs={'class':\"location accessible-contrast-color-location\"})\n",
    "        except:\n",
    "            location.append('Canada')\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#can\n",
    "for x in tqdm(range(10,3600,10)):\n",
    "    anal = url_13.format(x)\n",
    "    r_13 = requests.get(anal)\n",
    "    soup = BeautifulSoup(r_13.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('div', attrs={'class':\"location accessible-contrast-color-location\"})\n",
    "        except:\n",
    "            location.append('Canada')\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "\n",
    "#London\n",
    "for x in tqdm(range(10,7000,10)):\n",
    "    ldn = url_14.format(x)\n",
    "    r_14 = requests.get(ldn)\n",
    "    soup = BeautifulSoup(r_14.text, 'html.parser')\n",
    "    #Start from here to add the cards\n",
    "    for item in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        #This will be for the job title\n",
    "        try:\n",
    "            job_title.append(item.find('h2', attrs={'class':'title'}).text)\n",
    "        except:\n",
    "            job_title.append(np.nan)\n",
    "        #this will be for location\n",
    "        try:\n",
    "            location.append('div', attrs={'class':\"location accessible-contrast-color-location\"})\n",
    "        except:\n",
    "            location.append('London')\n",
    "        #This will be for the date it was posted    \n",
    "        try:\n",
    "            date_posted.append(item.find('span', attrs={'class':'date'}).text)\n",
    "        except:\n",
    "            date_posted.append(np.nan)\n",
    "        try:\n",
    "            salary.append(item.find('span', attrs={'class':'salaryText'}).text)\n",
    "        except:\n",
    "            salary.append(np.nan)\n",
    "        try:\n",
    "            company.append(item.find('a', attrs={'data-tn-element':'companyName'}).text)\n",
    "        except:\n",
    "            company.append(np.nan)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame from the scraped values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Job_Title' : job_title,\n",
    "                 'Location' : location,\n",
    "                 'Date_Posted': date_posted,\n",
    "                  'Salary': salary,\n",
    "                  'Company': company})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('scraped_data_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to clean the data that has been scraped\n",
    "There is a need to clean this data with regex to allow for good models to be created down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use some RegEx to clean the scraped data into something more readable\n",
    "df = df.replace(r'\\n',  '', regex=True)\n",
    "df = df.replace(r'new', '', regex=True)\n",
    "#Save this csv as a raw file to come back to later if needed.\n",
    "df_1 = df.copy()\n",
    "df.to_csv(r'/Users/amitchandna/Documents/Data_Science/Github/Web_Scraping_Indeed/scraped_data_file')\n",
    "#Go on and drop the duplicates and the nan values from the scrape and print out the shape to get an idea of whats left.\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "#Save this as a cleaner version of the scraped data file.\n",
    "df_1.to_csv(r'/Users/amitchandna/Documents/Data_Science/Github/Web_Scraping_Indeed/clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Salary column by taking the average of the salaries that are provided for the various jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the Salary column by taking the average of the salaries that are provided for the various jobs.\n",
    "import re\n",
    "def sal_to_num(salary):\n",
    "    sal = int\n",
    "    '''Takes in a salary entry (as string) and extracts the digits as an integer.\n",
    "    If the salary is in a range (eg 20,000-25,00) it returns the average'''\n",
    "    \n",
    "    try:\n",
    "        sal = int(''.join(re.findall(r'([\\d-])', salary)))\n",
    "    except:\n",
    "        sal = (int(''.join(re.findall(r'([\\d-])', salary)).split(\"-\")[0]) + int(''.join(re.findall(r'([\\d-])', salary)).split(\"-\")[1])) / 2\n",
    "    \n",
    "    \n",
    "    return int(sal)\n",
    "        \n",
    "df.loc[:,'Salary'] = df.Salary.map(lambda x: sal_to_num(x)).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to adjust salaries into a common currency\n",
    "All values will be converted into USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the larger data frame into smaller ones to adjust the salaries accordingly.\n",
    "American = ['USA', 'Seattle WA', 'Austin TX', 'San Francisco CA']\n",
    "British = ['London']\n",
    "Canadian = ['Vancouver BC', 'Toronto ON', 'Canada', 'Montreal QC']\n",
    "Indian = ['Bangalore KA', 'Mumbai MH']\n",
    "\n",
    "American_df = df[df.Location.isin(American)]\n",
    "British_df = df[df.Location.isin(British)]\n",
    "Canadian_df = df[df.Location.isin(Canadian)]\n",
    "Indian_df = df[df.Location.isin(Indian)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to add a column that depicts each of the salaries in CAD rather than the various currencies they come from.\n",
    "ep = EasyPeasy()\n",
    "def cad_val(country_from, df):\n",
    "    currencies = []\n",
    "    for i in df['Salary']:\n",
    "        currencies.append(ep.currency_converter(amount = i, from_currency=country_from, to_currency=\"CAD\",))\n",
    "    df['CAD'] = currencies\n",
    "#Call the function on each of the individual country dataframes.\n",
    "cad_val('USA', American_df)\n",
    "cad_val('CAD', Canadian_df)\n",
    "cad_val('GBP', British_df)\n",
    "cad_val('INR', Indian_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in the Purchasing Price Parity (PPP) column to further adjust the salaries to something useful for comparison (Values adjusted as seen by OECD Data (https://data.oecd.org/conversion/purchasing-power-parities-ppp.htm) on 7.12.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PPP for India\n",
    "ppp = []\n",
    "for i in Indian_df.CAD:\n",
    "    ppp.append(21.107*(1.194*i))\n",
    "Indian_df['Adj_PPP_CAD'] = ppp   \n",
    "#PPP for London\n",
    "ppp = []\n",
    "for i in British_df.CAD:\n",
    "    ppp.append(0.68*(1.194*i))\n",
    "British_df['Adj_PPP_CAD'] = ppp    \n",
    "#PPP for United States\n",
    "ppp = []\n",
    "for i in American_df.CAD:\n",
    "    ppp.append(i)\n",
    "American_df['Adj_PPP_CAD'] = ppp   \n",
    "#PPP for Canada\n",
    "ppp = []\n",
    "for i in Canadian_df.CAD:\n",
    "    ppp.append(1.194*i)\n",
    "Canadian_df['Adj_PPP_CAD'] = ppp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restitch the frames together now to get the main dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [Canadian_df, American_df, British_df, Indian_df]\n",
    "df = pd.concat(frames)\n",
    "#Drop the useless residual columns\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "#Take a profile report of what the dataframe looks like now\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_salary = df.CAD.median()\n",
    "#In this function the value of x is the median salary and the function checks whether or not\n",
    "#the value is above it. If X is higher than the median salary then boolean value of True is returned.\n",
    "def am_i_good(x = df.CAD):\n",
    "    my_bool = False\n",
    "    if x > median_salary:\n",
    "        my_bool=True\n",
    "    return my_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for i in range(0, len(df.CAD),1):\n",
    "    hold = []\n",
    "    hold = am_i_good(df.CAD[i])\n",
    "    new_list.append(hold)\n",
    "df['greater_than_median_val'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have a final dataframe - it can be loaded and played with for the future.\n",
    "df.to_csv(r'/Users/amitchandna/Documents/Data_Science/Github/Web_Scraping_Indeed/CleanData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CleanData')\n",
    "df = df.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the target and predictor variables\n",
    "X = df['Location']\n",
    "y = df.pop('greater_than_median_val')\n",
    "#Get Dummies\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch to Greatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the models for testing purposes\n",
    "model_lr = LogisticRegression(max_iter=10000)\n",
    "gs_lr_params = {'penalty': ['l1', 'l2'],\n",
    "                   'solver': ['liblinear'],\n",
    "                   'C': np.logspace(-4, 2, 100)}\n",
    "\n",
    "\n",
    "model_svc = LinearSVC(max_iter=10000)\n",
    "gs_svc_params = {'fit_intercept': [True, False],\n",
    "                   'loss': ['hinge', 'squared_hinge'],\n",
    "                   'C': np.logspace(-4, 2, 100),\n",
    "                   'penalty': ['l1', 'l2']}\n",
    "\n",
    "\n",
    "model_dt = DecisionTreeClassifier()\n",
    "gs_dt_params = {'max_depth': range(1,10),\n",
    "                'max_features': ['sqrt', 'log2', None],\n",
    "                }\n",
    "\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "gs_rf_params = {'n_estimators':[10, 50, 100],\n",
    "                'max_depth': range(1,8),\n",
    "                'max_features':[2,4,6, 0.5,'auto',None],\n",
    "                'max_leaf_nodes': [5, 7, 10, None],\n",
    "                #'min_samples_leaf': [1, 2, 3, 4],\n",
    "                }\n",
    "\n",
    "model_ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "gs_ada_params = {'n_estimators': [50, 200],\n",
    "                 'learning_rate': [0.2, 0.5, 1],\n",
    "                 'base_estimator__max_depth' : [1, 2, 3],\n",
    "                 'base_estimator__max_features' : [1,2,3,None]\n",
    "                 }\n",
    "\n",
    "\n",
    "model_gbc = GradientBoostingClassifier()\n",
    "gs_gbc_params = {'learning_rate':[0.01,0.5,1.0],\n",
    "                 'n_estimators': [200],    \n",
    "                 'max_depth': [1, 2],\n",
    "                 'max_features': ['sqrt', None],\n",
    "                }\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_bagging = BaggingClassifier(base_estimator=knn, n_estimators=100)\n",
    "gs_bagging_params = {'max_samples': np.linspace(0.8, 1.0, 3),\n",
    "          'max_features': range(int(3/4.*X.shape[1]), X.shape[1]+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue setting up a GridSearchCV on each of the models above to determine the best params and then run the models.\n",
    "gs_lr = GridSearchCV(model_lr, gs_lr_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_svc = GridSearchCV(model_svc, gs_svc_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_dt = GridSearchCV(model_dt, gs_dt_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_rf = GridSearchCV(model_rf, gs_rf_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_ada = GridSearchCV(model_ada, gs_ada_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_gbc = GridSearchCV(model_gbc, gs_gbc_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_bag = GridSearchCV(model_bagging, gs_bagging_params, cv = 5, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the GridSearch operations on the training data set.\n",
    "gs_lr.fit(X_train, y_train);\n",
    "gs_svc.fit(X_train, y_train);\n",
    "gs_dt.fit(X_train, y_train);\n",
    "gs_rf.fit(X_train, y_train);\n",
    "gs_ada.fit(X_train, y_train);\n",
    "gs_gbc.fit(X_train, y_train);\n",
    "gs_bag.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the best_estimator_ in a new variable\n",
    "gs_best_lr = gs_lr.best_estimator_\n",
    "gs_best_svc = gs_svc.best_estimator_\n",
    "gs_best_dt = gs_dt.best_estimator_\n",
    "gs_best_rf = gs_rf.best_estimator_\n",
    "gs_best_ada = gs_ada.best_estimator_\n",
    "gs_best_gbc = gs_gbc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scoreboard to demonstrate which model is best and print out the corresponding scores.\n",
    "gs_models = {'Logistic Regression':gs_best_lr, \n",
    "             'Support Vector Classifier':gs_best_svc, \n",
    "             'Decision Tree Classifier':gs_best_dt, \n",
    "             'Random Forest Classifier': gs_best_rf,\n",
    "             'Adaboost (Dec. Tree)':gs_best_ada,\n",
    "             'Grad. Boosting Classifier':gs_best_gbc,\n",
    "            'Bagging Classifier':gs_best_bag}\n",
    "scores = []\n",
    "\n",
    "for name, model in gs_models.items():\n",
    "    scores.append((name, model.score(X_train, y_train),\n",
    "    cross_val_score(model, X_train, y_train, cv=5).mean(),model.score(X_test, y_test)))\n",
    "\n",
    "model_df = pd.DataFrame(scores, columns = ['Model', 'Training_Score','CV_Score','Test_Score'])\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Random Forest is underfit to provide less accuracy\n",
    "Underfit = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                        criterion='gini', max_depth=8, max_features=0.5,\n",
    "                        max_leaf_nodes=20, max_samples=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "                        n_jobs=None, oob_score=False, random_state=None,\n",
    "                        verbose=0, warm_start=False)\n",
    "Underfit.fit(X_train,y_train)\n",
    "Underfit.score(X_train,y_train)\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Logistic regression is also underfit to provide less accuracy\n",
    "lr_underfit = LogisticRegression(C=2.310129700083158, class_weight=None, dual=False,\n",
    "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                    max_iter=10000, multi_class='auto', n_jobs=None,\n",
    "                    penalty='l1', random_state=None, solver='liblinear',\n",
    "                    tol=0.0001, verbose=0, warm_start=False)\n",
    "lr_underfit.fit(X_train,y_train)\n",
    "lr_underfit.score(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
